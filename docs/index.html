<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="View Invariant Learning for Vision-Language Navigation in Continuous Environments">
  <meta property="og:title" content="View Invariant Learning for Vision-Language Navigation in Continuous Environments"/>
  <meta property="og:description" content="IEEE RA-L accepted paper"/>
  <meta property="og:url" content="https://realjoshqsun.github.io/V2-VLNCE/"/>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>View Invariant Learning for Vision-Language Navigation in Continuous Environments</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <script defer src="static/js/fontawesome.all.min.js"></script>
</head>

<body>

<section class="hero">
<div class="column has-text-centered">

  <h1 class="title is-1 publication-title">
    View Invariant Learning for Vision-Language Navigation in Continuous Environments
  </h1>

  <div class="is-size-5 publication-authors">
    <span class="author-block">
      Josh Qixuan Sun<sup>1*</sup>,
    </span>
    <span class="author-block">
      Huaiyuan Weng<sup>2</sup>,
    </span>
    <span class="author-block">
      Xiaoying Xing<sup>3</sup>,
    </span>
    <span class="author-block">
      Chul Min Yeum<sup>2</sup>,
    </span>
    <span class="author-block">
      Mark Crowley<sup>1</sup>
    </span>
  </div>

  <div class="is-size-6" style="margin-top: 0.5rem;">
    <div>
      <sup>1</sup> Department of Electrical and Computer Engineering, University of Waterloo, Canada
    </div>
    <div>
      <sup>2</sup> Department of Civil and Environmental Engineering, University of Waterloo, Canada
    </div>
    <div>
      <sup>3</sup> Department of Electrical and Computer Engineering, Northwestern University, USA
    </div>
    <div style="margin-top: 0.5rem;">
      <sup>*</sup> Corresponding Author
    </div>
  </div>

  <div class="publication-links" style="margin-top: 1rem;">

    <!-- arXiv -->
    <span class="link-block">
      <a href="https://arxiv.org/abs/2507.08831"
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
          <i class="ai ai-arxiv"></i>
        </span>
        <span>arXiv</span>
      </a>
    </span>

    <!-- GitHub -->
    <span class="link-block">
      <a href="https://github.com/realjoshqsun/V2-VLNCE"
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
          <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
      </a>
    </span>

  </div>

  <div style="margin-top: 1rem;">
    <strong>Accepted to IEEE Robotics and Automation Letters (RA-L) 2026</strong>
  </div>

</div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="content has-text-justified">
      <h2 class="title is-3 has-text-centered">Abstract</h2>
      <p>
Vision-Language Navigation in Continuous Environments (VLNCE), where an agent follows instructions and moves freely to reach a destination, is a key research problem in embodied AI. However, most existing approaches are sensitive to viewpoint changes, i.e. variations in camera height and viewing angle. Here we introduce a more general scenario, V²-VLNCE (VLNCE with Varied Viewpoints) and propose a view-invariant post-training framework, called VIL (View Invariant Learning), that makes existing navigation policies more robust to changes in camera viewpoint. VIL employs a contrastive learning framework to learn sparse and view-invariant features. We also introduce a teacher-student framework for the Waypoint Predictor Module, where a view-dependent teacher model distills knowledge into a view-invariant student model. Empirical results show that our method outperforms state-of-the-art approaches on V²-VLNCE by 8–15% measured on Success Rate for R2R-CE and RxR-CE. We further evaluate VIL on simulated robot camera placements and real-world robot navigation, showing consistent improvements in robustness.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3 has-text-centered">V²-VLNCE Benchmark</h2>

    <div class="content has-text-justified">
      <p>
The <em>Varied Viewpoint</em> challenge occurs when agents must generalize across environments with different egocentric camera placements, which is common in real-world robotics due to differences in sensor mounting. To systematically study this problem, we introduce <em>V²-VLNCE (VLNCE with Varied Viewpoints)</em>, a generalized evaluation setting designed to measure navigation performance under diverse camera viewpoints.
      </p>

      <p>
We focus on two viewpoint variables: camera height and camera angle. For each navigation episode, a viewpoint is sampled from a two-dimensional distribution over height and angle ranges. This design better reflects realistic deployment conditions compared to fixed-viewpoint VLNCE settings.
      </p>
    </div>

    <div class="has-text-centered">
      <img src="static/images/v2vlnce_benchmark.png">
    </div>

    <div class="content has-text-centered">
      <p>
Comparison between standard VLNCE and our proposed V²-VLNCE. Under viewpoint changes, baseline navigation policies suffer from degraded performance. Applying View Invariant Learning (VIL) improves robustness and enables stable navigation across varied viewpoints.
      </p>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3 has-text-centered">Approach</h2>

    <div class="content has-text-justified">
      <p>
We propose <em>View Invariant Learning (VIL)</em>, a strategy that adapts existing navigation policies to varied viewpoints without retraining from scratch. VIL contains two components: a contrastive learning objective and a teacher-student framework for waypoint prediction.
      </p>

      <p>
The contrastive component encourages sparse and view-invariant representations by aligning observations of the same scene captured from different viewpoints while separating unrelated observations. Features for contrastive learning are produced by a projection head and are shared with the navigation policy.
      </p>

      <p>
For waypoint prediction, a frozen teacher model, initialized from a pretrained navigation policy, processes observations from a standard viewpoint. The student model shares the same architecture but trains only a lightweight adapter inserted into the waypoint predictor while keeping the remaining parameters frozen. The student receives varied-viewpoint observations and learns to match the teacher outputs through a distillation loss.
      </p>

      <p>
Both components are optimized jointly in an end-to-end manner, enabling efficient adaptation to viewpoint variation while preserving the original navigation capability.
      </p>
    </div>

    <div class="has-text-centered">
      <img src="static/images/VIL-3.png">
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3 has-text-centered">Quantitive Results</h2>

    <h3 class="title is-4">Results on V²-VLNCE</h3>
    <div class="has-text-centered">
      <img src="static/images/result1.png">
      <p>
Comparison on R2R-CE and RxR-CE under the <em>Varied Viewpoint</em> and <em>Ground-level Viewpoint</em> settings. The <em>Varied Viewpoint</em> setting corresponds to our proposed V²-VLNCE setup. The <em>Ground-level Viewpoint</em> setting is adapted from GVNav. Evaluation metrics are consistent with GVNav. Bold indicates performance improvements introduced by VIL.
      </p>
    </div>


    <h3 class="title is-4">Results on VLNCE</h3>
    <div class="has-text-centered">
      <img src="static/images/result2.png" width="70%">
      <p>
Performance under standard viewpoint (VLNCE).
      </p>
    </div>

  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Qualitative Comparisons</h2>
      <p>
Camera viewpoint (position) configuration: From the perspective of a Stretch RE-1 robot. View: Due to the geometric variations in camera height and pitch angle, all videos display the Front-View (egocentric RGB) to provide the most accurate representation of the agent's visual input.
      </p>

    <h3 class="title is-4">ETPNav vs VIL-ETPNav</h3>

    <div class="columns">
      <div class="column">
        <video controls autoplay muted loop>
          <source src="static/videos/Case1_ETPNav.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered">Case 1 — ETPNav</p>
      </div>
      <div class="column">
        <video controls autoplay muted loop>
          <source src="static/videos/Case1_VIL-ETPNav.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered">Case 1 — VIL-ETPNav</p>
      </div>
    </div>

    <div class="columns">
      <div class="column">
        <video controls autoplay muted loop>
          <source src="static/videos/Case2_ETPNav.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered">Case 2 — ETPNav</p>
      </div>
      <div class="column">
        <video controls autoplay muted loop>
          <source src="static/videos/Case2_VIL-ETPNav.mp4" type="video/mp4">
        </video>
	<p class="has-text-centered">Case 2 — VIL-ETPNav</p>
      </div>
    </div>

    <div class="columns">
      <div class="column">
        <video controls autoplay muted loop>
          <source src="static/videos/Case3_ETPNav.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered">Case 3 — ETPNav</p>
      </div>
      <div class="column">
        <video controls autoplay muted loop>
          <source src="static/videos/Case3_VIL-ETPNav.mp4" type="video/mp4">
        </video>
	<p class="has-text-centered">Case 3 — VIL-ETPNav</p>
      </div>
    </div>

    <h3 class="title is-4">BEVBert vs VIL-BEVBert</h3>

    <div class="columns">
      <div class="column">
        <video controls autoplay muted loop>
          <source src="static/videos/Case4_BEVBert.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered">Case 4 — BEVBert</p>
      </div>
      <div class="column">
        <video controls autoplay muted loop>
          <source src="static/videos/Case4_VIL-BEVBert.mp4" type="video/mp4">
        </video>
	<p class="has-text-centered">Case 4 — VIL-BEVBert</p>
      </div>
    </div>

    <div class="columns">
      <div class="column">
        <video controls autoplay muted loop>
          <source src="static/videos/Case5_BEVBert.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered">Case 5 — BEVBert</p>
      </div>
      <div class="column">
        <video controls autoplay muted loop>
          <source src="static/videos/Case5_VIL-BEVBert.mp4" type="video/mp4">
        </video>
	<p class="has-text-centered">Case 5 — VIL-BEVBert</p>
      </div>
    </div>

    <div class="columns">
      <div class="column">
        <video controls autoplay muted loop>
          <source src="static/videos/Case6_BEVBert.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered">Case 6 — BEVBert</p>
      </div>
      <div class="column">
        <video controls autoplay muted loop>
          <source src="static/videos/Case6_VIL-BEVBert.mp4" type="video/mp4">
        </video>
	<p class="has-text-centered">Case 6 — VIL-BEVBert</p>
      </div>
    </div>

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@article{sun2025view,
  title={View invariant learning for vision-language navigation in continuous environments},
  author={Sun, Josh Qixuan and Xing, Xiaoying and Weng, Huaiyuan and Yeum, Chul Min and Crowley, Mark},
  journal={arXiv preprint arXiv:2507.08831},
  year={2025}
}</code></pre>
  </div>
</section>

</body>
</html>
