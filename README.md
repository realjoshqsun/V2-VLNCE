# View Invariant Learning for Vision-Language Navigation in Continuous Environments

[![Home Page](https://img.shields.io/badge/Homepage-V2_VLNCE-144B9E.svg)](https://arxiv.org/pdf/2507.08831)
[![arXiv](https://img.shields.io/badge/Arxiv-V2_VLNCE-b31b1b.svg?logo=arXiv)](https://arxiv.org/pdf/2507.08831)

## Core Highlights
ü§è $V^2$-VLNCE Benchmark Integration: Effortlessly extend any standard VLNCE benchmark to the more challenging Varied Viewpoint ($V^2$) scenario with just a few lines of code. No new datasets required. 

üß¨ View-Invariant Learning (VIL): A plug-and-play post-training framework that learns sparse, viewpoint-invariant features through a novel contrastive learning objective. 

üéì Teacher-Student Distillation: Features a specialized distillation framework for the Waypoint Predictor, where a view-dependent teacher guides a student model to maintain robust navigation under drastic camera shifts. 

üöÄ State-of-the-Art Performance: Outperforms existing baselines on $V^2$-VLNCE by 8-15% in Success Rate (SR) across R2R-CE and RxR-CE, while also improving performance in standard settings

## üî• News
| Time   | Update |
|---------|--------|
| Feb 17 2026 | This paper is accepeted to IEEE Robotics and Automation Letters (RA-L) 2026! |

## Citation

If you find this work useful, please cite:

```bibtex
@article{sun2025view,
  title={View invariant learning for vision-language navigation in continuous environments},
  author={Sun, Josh Qixuan and Xing, Xiaoying and Weng, Huaiyuan and Yeum, Chul Min and Crowley, Mark},
  journal={arXiv preprint arXiv:2507.08831},
  year={2025}
}

